{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ko8K6ntRi-W6"},"source":["# Homework: Word Embedding\n","\n","In this exercise, you will work on the skip-gram neural network architecture for Word2Vec. You will be using Keras to train your model. \n","\n","You must complete the following tasks:\n","1. Read/clean text files\n","2. Indexing (Assign a number to each word)\n","3. Create skip-grams (inputs for your model)\n","4. Create the skip-gram neural network model\n","5. Visualization\n","6. Evaluation (Using pre-trained, not using pre-trained)  \n","    (classify topic from 4 categories) \n","    \n","This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3351,"status":"ok","timestamp":1677157024488,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Jw11OhLsi-W8","outputId":"9bc4fab2-9292-432d-e10a-b455baedc0bd"},"outputs":[],"source":["# %tensorflow_version 2.x\n","%matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import math\n","import glob\n","import re\n","import random\n","import collections\n","import os\n","import sys\n","import tensorflow as tf\n","from keras.preprocessing import sequence\n","from keras.models import Sequential, Model\n","from keras.layers import GRU, Dropout\n","from keras.models import load_model\n","from keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking, Conv1D, Bidirectional\n","from tensorflow.python.keras.layers.merge import Dot\n","from tensorflow.python.keras.utils import np_utils\n","from tensorflow.python.keras.utils.data_utils import get_file\n","from tensorflow.python.keras.utils.np_utils import to_categorical\n","from keras.preprocessing.sequence import skipgrams\n","from keras.preprocessing import sequence\n","from keras import backend as K\n","from keras.optimizers import Adam\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","mpl.font_manager.fontManager.addfont('../../font/THSarabunNew.ttf')\n","mpl.rc('font', family='TH Sarabun New')\n","\n","random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"RdYpL3Uyi-XD"},"source":["# Step 1: Read/clean text files\n","\n","The given code can be used to processed the pre-tokenzied text file from the wikipedia corpus. In your homework, you must replace those text files with raw text files.  You must use your own tokenizer to process your text files"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31077,"status":"ok","timestamp":1677157091968,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"28fKcXgoU-vB","outputId":"81545049-40cb-4376-bbc7-b0bb0aa2b9a9"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":2122,"status":"ok","timestamp":1677157102116,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Wco1eVRVzn6O","outputId":"f7c716fb-db57-4032-b763-1db27b849945"},"outputs":[],"source":["# import shutil\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/wiki.zip\",\"/content/wiki.zip\")\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/BEST-TrainingSet.zip\",\"/content/BEST-TrainingSet.zip\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4562,"status":"ok","timestamp":1677157130039,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"BUW1SwAkUIBo","outputId":"eaec628f-abfa-456b-91f8-4d165cc3a403"},"outputs":[],"source":["# !unzip wiki.zip\n","# !unzip BEST-TrainingSet.zip"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"s0ALwvtzDZ-f"},"outputs":[],"source":["#Step 1: read the wikipedia text file\n","with open(\"content/unzipped/wiki/thwiki_chk.txt\") as f:\n","    #the delimiter is one or more whitespace characters\n","    input_text = re.compile(r\"\\s+\").split(f.read()) \n","    #exclude an empty string from our input\n","    input_text = [word for word in input_text if word != ''] "]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677157158218,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"KXoFAfjaDcJ2","outputId":"189dc5eb-a62b-41c6-e4e5-1756bf95f40a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['หน้า', 'หลัก', 'วิกิพีเดีย', 'ดำเนินการ', 'โดย', 'มูลนิธิ', 'วิกิ', 'มีเดีย', 'องค์กร', 'ไม่']\n","total word count: 36349066\n"]}],"source":["tokens = input_text\n","print(tokens[:10])\n","print(\"total word count:\", len(tokens))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GDVMvTRci-Xu"},"source":["# Step 2: Indexing (Assign a number to each word)\n","\n","The code below generates an indexed dataset(each word is represented by a number), a dictionary, a reversed dictionary\n","\n","## <font color='salmon'>Homework Question 1:</font>\n","<font color='salmon'>“UNK” is often used to represent an unknown word (a word which does not exist in your dictionary/training set). You can also represent a rare word with this token as well.  How do you define a rare word in your program? Explain in your own words and capture the screenshot of your code segment that is a part of this process</font>\n","\n"," + <font color='salmon'>edit or replace create_index with your own code to set a threshold for rare words and replace them with \"UNK\"</font>\n","\n","\n","### **QUESTION 1 ANSWER:**\n","\n","![Question 1 Code](https://github.com/AthimetA/FRA501_Introduction_To_Natural_Language_Processing_With_Deep_Learning/blob/main/pic/hw2-2/qn1-code1.png?raw=true)\n","\n","From the picture, the first line of code will give us a descending list of unique words sorted by frequency of the word.\n","\n","This means that if we make a loop that scans through the list, we will be able find the index of the first unique word with the frequency less than or equals to the minimum threshold for unknown words (min_thres_unk) then break the loop right away after getting the index of the first unique word.\n","\n","After that, we will remove all the unique words with frequency less than or equals to the minimum threshold for unknown words by slicing the list with the index of the first unique word.\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18268,"status":"ok","timestamp":1677158042562,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"O6NP7nQGi-Xw","outputId":"80d3e390-d512-49dd-adcc-031a94db7df8"},"outputs":[],"source":["#step 2:Build dictionary and build a dataset(replace each word with its index)\n","def create_index(input_text, min_thres_unk = 1, max_word_count = None, debug=False):\n","    # TODO#1 : edit or replace this function\n","    debugprint = print if debug else lambda *a, **k: None\n","    words = [word for word in input_text]\n","    debugprint(\"total word count:\", len(words))\n","    word_count = list()\n","    \n","    #use set and len to get the number of unique words\n","    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n","    # print(\"number of words with frequency less than\", min_thres_unk, \":\", len(minkeys))\n","    for word in word_count:\n","        if word[1] <= min_thres_unk:\n","            first_index = word_count.index(word)\n","            debugprint(f\"first index of word with frequency less than {min_thres_unk} is {first_index}\")\n","            debugprint(f\"length of word_count is {len(word_count)}\")\n","            break\n","    word_count = word_count[:first_index]\n","    #include a token for unknown word\n","    word_count.append((\"UNK\", len(words) - sum([word[1] for word in word_count])))\n","    debugprint(\"total unique word count:\", len(word_count))\n","    debugprint(\"total word count after removing words with frequency less than or equal to\", min_thres_unk, \":\", sum([word[1] for word in word_count]) - word_count[-1][1])\n","\n","    dictionary = dict()\n","    dictionary[\"for_keras_zero_padding\"] = 0\n","    for word in word_count:\n","        dictionary[word[0]] = len(dictionary)\n","    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n","    data = list()\n","    for word in input_text:\n","        if word in dictionary:\n","            data.append(dictionary[word])\n","    \n","    #print out 10 most frequent words\n","    debugprint(\"first 10 words\", word_count[:10])\n","    debugprint(\"last 10 words\", word_count[-10:])\n","    debugprint(\"total unique word count:\", len(word_count))\n","    # with open(\"word_count.txt\", \"w\") as f:\n","    #     f.write(str(word_count))\n","\n","    return data, dictionary, reverse_dictionary"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["35942874\n","295164\n"]}],"source":["# call method with min_thres_unk=1ß\n","dataset, dictionary, reverse_dictionary = create_index(tokens, 1)\n","print(len(dataset))\n","print(len(dictionary))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1677158042563,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"2fotaYMgi-Xz","outputId":"065bb577-ce57-40d1-d2c6-cf2b716a9b34"},"outputs":[{"name":"stdout","output_type":"stream","text":["output sample (dataset): [229, 208, 2453, 573, 15, 1829, 7149, 3124, 681, 24]\n","output sample (dictionary): {'for_keras_zero_padding': 0, 'ที่': 1, 'ใน': 2, 'เป็น': 3, 'และ': 4, 'การ': 5, 'มี': 6, 'ของ': 7, 'ได้': 8, ')': 9}\n","output sample (reverse dictionary): {0: 'for_keras_zero_padding', 1: 'ที่', 2: 'ใน', 3: 'เป็น', 4: 'และ', 5: 'การ', 6: 'มี', 7: 'ของ', 8: 'ได้', 9: ')'}\n"]}],"source":["print(\"output sample (dataset):\",dataset[:10])\n","print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:10]})\n","print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:10]})"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["with open(\"debug_dictionary.txt\", \"w\") as f:\n","    f.write(str(dictionary))\n","\n","with open(\"debug_reverse_dictionary.txt\", \"w\") as f:\n","    f.write(str(reverse_dictionary))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HutTzPO7i-X3"},"source":["# Step3: Create skip-grams (inputs for your model)\n","Keras has a skipgrams-generator, the cell below shows us how it generates skipgrams \n","\n","## <font color='salmon'>Homework Question 2:</font>\n","<font color='salmon'>The negative samples are sampled from sampling_table.  Look through Keras source code to find out how they sample negative samples. Discuss the sampling technique taught in class and compare it to the Keras source code.</font>\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cwYFRO3YGryQ"},"source":["<font color='red'>**ANS Q2:**</font>  \n","In class, we learned that negative sampling is when we sample words that is not one of the context words for our target word from the whole vocabulary dictionary (list of words) that we have. The way we sample it is by using probabilities of the appearance of the words in a sentence, the lower the probability is the better negative sample it is.\n","\n","From Keras source code, we uses the function \"make_sampling_table\" to generate the sampling argument for skipgram with the same idea as what we learned in class, **more common words should be sampled less frequently.** We uses this function as a parameter inside of \"skipgram\" function called \"sampling_table\"."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677158250675,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"C520WnI0i-X4","outputId":"530b4f53-971e-4a00-8058-f9874e5ca017"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[208, 83708], [208, 2453], [2453, 573], [208, 235515], [24, 681], [2453, 208], [3124, 145853], [2453, 115575], [24, 3408], [208, 229], [3124, 681], [3124, 7149], [3124, 285707], [2453, 219950]] [0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n","หลัก Merlin\n","หลัก วิกิพีเดีย\n","วิกิพีเดีย ดำเนินการ\n","หลัก LaVoie\n","ไม่ องค์กร\n","วิกิพีเดีย หลัก\n","มีเดีย -House\n","วิกิพีเดีย (หงส์ทอง\n"]}],"source":["# Step 3: Create data samples\n","vocab_size = len(dictionary)\n","skip_window = 1       # How many words to consider left and right.\n","\n","# TODO#2 check out keras source code and find out how their sampling technique works. Describe it in your own words.\n","sample_set= dataset[:10]\n","sampling_table = sequence.make_sampling_table(vocab_size)\n","couples, labels = skipgrams(sample_set, vocab_size, window_size=skip_window, sampling_table=sampling_table)\n","word_target, word_context = zip(*couples)\n","word_target = np.array(word_target, dtype=\"int32\")\n","word_context = np.array(word_context, dtype=\"int32\")\n","\n","print(couples, labels)\n","\n","for i in range(8):\n","    print(reverse_dictionary[couples[i][0]],reverse_dictionary[couples[i][1]])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F6UL0FhEi-X6"},"source":["# Step 4: create the skip-gram model\n","## <font color='salmon'>Homework Question 3:</font>\n"," <font color='salmon'>Q3:  In your own words, discuss why Sigmoid is chosen as the activation function in the  skip-gram model.</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-oQLGkkuHG7o"},"source":["<font color='red'>**ANS Q3:**</font>  \n","The skip-gram model predicts the probability of each context words. Therefore, the output is in between 0 and 1 and since Sigmoid activation function's range of answer is in between 0 and 1, this means that Sigmoid fits perfectly as the activation function for the skip-gram model."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5715,"status":"ok","timestamp":1677158262805,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"kq7Eh9pXi-X7","outputId":"f4bd6052-53df-4c40-e81c-562eb2fd5f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," input_1 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 1, 32)        9445280     ['input_2[0][0]']                \n","                                                                                                  \n"," embedding (Embedding)          (None, 1, 32)        9445280     ['input_1[0][0]']                \n","                                                                                                  \n"," tf.compat.v1.transpose (TFOpLa  (None, 32, 1)       0           ['embedding_1[0][0]']            \n"," mbda)                                                                                            \n","                                                                                                  \n"," tf.linalg.matmul (TFOpLambda)  (None, 1, 1)         0           ['embedding[0][0]',              \n","                                                                  'tf.compat.v1.transpose[0][0]'] \n","                                                                                                  \n"," reshape (Reshape)              (None, 1)            0           ['tf.linalg.matmul[0][0]']       \n","                                                                                                  \n"," activation (Activation)        (None, 1)            0           ['reshape[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 18,890,560\n","Trainable params: 18,890,560\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["b:\\miniconda\\envs\\tf_ml\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]}],"source":["#reference: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n","dim_embedddings = 32\n","V= len(dictionary)\n","\n","#step1: select the embedding of the target word from W\n","w_inputs = Input(shape=(1, ), dtype='int32')\n","w = Embedding(V+1, dim_embedddings)(w_inputs)\n","\n","#step2: select the embedding of the context word from C\n","c_inputs = Input(shape=(1, ), dtype='int32')\n","c  = Embedding(V+1, dim_embedddings)(c_inputs)\n","\n","#step3: compute the dot product:c_k*v_j\n","o = Dot(axes=2)([w, c])\n","o = Reshape((1,), input_shape=(1, 1))(o)\n","\n","#step4: normailize dot products into probability\n","o = Activation('sigmoid')(o)\n","#TO DO#4 Question: Why sigmoid?\n","\n","SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n","SkipGram.summary()\n","opt=Adam(lr=0.01)\n","SkipGram.compile(loss='binary_crossentropy', optimizer=opt)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"MgR5p_h1i-X9"},"outputs":[],"source":["# # you don't have to spend too much time training for your homework, you are allowed to do it on a smaller corpus\n","# # currently the dataset is 1/20 of the full text file.\n","# for _ in range(5):\n","#     prev_i=0\n","#     #it is likely that your GPU won't be able to handle large input\n","#     #just do it 100000 words at a time\n","#     for i in range(len(dataset)//100000):\n","#         #generate skipgrams\n","#         data, labels = skipgrams(sequence=dataset[prev_i*100000:(i*100000)+100000], vocabulary_size=V, window_size=2, negative_samples=4.)\n","#         x = [np.array(x) for x in zip(*data)]\n","#         y = np.array(labels, dtype=np.int32)\n","#         if x:\n","#             loss = SkipGram.train_on_batch(x, y)\n","#         prev_i = i \n","#         print(loss,i*100000)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"sY69_WFHi-X_"},"outputs":[],"source":["# SkipGram.save_weights('hw2_2_skipgram_weight.h5')"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"7UD13eKki-YA","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.02942849 -0.01651738  0.00090514 ... -0.0396731  -0.00961649\n","   0.0410727 ]\n"," [ 0.7198411   0.7680737  -0.78755    ... -0.7289395   0.7654709\n","  -0.7568815 ]\n"," [ 0.7776858   0.6873883  -0.73114496 ... -0.7483812   0.78560984\n","  -0.8015211 ]\n"," ...\n"," [-0.5841268  -0.66552055  0.6502833  ...  0.69298637 -0.6493391\n","   0.6393718 ]\n"," [-0.59900445 -0.6754135   0.5988361  ...  0.6239907  -0.6370992\n","   0.64077324]\n"," [ 0.1091732   0.16109465 -0.19907406 ... -0.200748    0.23554419\n","  -0.25209388]]\n","(295165, 32)\n"]}],"source":["#Get weight of the embedding layer\n","SkipGram.load_weights('hw2_2_skipgram_weight.h5')\n","final_embeddings=SkipGram.get_weights()[0]\n","print(final_embeddings)\n","print(final_embeddings.shape)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample word2vec: [('for_keras_zero_padding', array([ 0.02942849, -0.01651738,  0.00090514,  0.04732073, -0.01664939,\n","        0.0048499 ,  0.02143706, -0.01363739,  0.03147663,  0.02441362,\n","        0.00328835, -0.01535859,  0.02068813,  0.00460305, -0.02345526,\n","       -0.00876445, -0.03275287, -0.02459653, -0.00437919, -0.02083881,\n","        0.03819272,  0.01364623, -0.01814321,  0.04208614,  0.04154963,\n","       -0.04032252, -0.0279218 ,  0.03789641,  0.00210012, -0.0396731 ,\n","       -0.00961649,  0.0410727 ], dtype=float32)), ('ที่', array([ 0.7198411 ,  0.7680737 , -0.78755   ,  0.75145125,  0.29124954,\n","        0.18292074,  0.728924  , -0.64127713, -0.6566867 ,  0.5809269 ,\n","        0.7027372 ,  0.5736526 , -0.7075159 , -0.6948407 , -0.7013153 ,\n","       -0.500572  ,  0.43746254,  0.73615015, -0.8625655 , -0.7182525 ,\n","        0.76134247, -0.6409293 , -0.69192487, -0.6871512 , -0.71799767,\n","       -0.7479568 ,  0.8275282 , -0.6782602 ,  0.6947561 , -0.7289395 ,\n","        0.7654709 , -0.7568815 ], dtype=float32)), ('ใน', array([ 0.7776858 ,  0.6873883 , -0.73114496,  0.72385365,  0.23262656,\n","        0.25871938,  0.6528968 , -0.64551544, -0.65397066,  0.6124145 ,\n","        0.6771981 ,  0.7673169 , -0.7424297 , -0.5842767 , -0.69119245,\n","       -0.69496477,  0.28918707,  0.7373382 , -0.8899485 , -0.6593099 ,\n","        0.73211545, -0.68125564, -0.738861  , -0.611389  , -0.748133  ,\n","       -0.7597389 ,  0.63971025, -0.75350356,  0.6572468 , -0.7483812 ,\n","        0.78560984, -0.8015211 ], dtype=float32))]\n"]}],"source":["# Map ecach word to its embedding\n","word2vec = dict(zip(dictionary.keys(), final_embeddings))\n","print(f'Sample word2vec: {list(word2vec.items())[:3]}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8ovPmh6Ri-YC"},"source":["# Step 5: Intrinsic Evaluation: Word Vector Analogies\n","## <font color='salmon'>Homework Question 4: </font>\n","<font color='salmon'> Read section 2.1 and 2.3 in this [lecture note](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf). Come up with 10 semantic analogy examples and report results produced by your word embeddings. Discuss t-SNE in 2 dimensions. </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_with_labels(low_dim_embs, labels, filename=None):\n","    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n","    plt.figure(figsize=(15, 15))  # in inches\n","    plt.axis('off')\n","    for i, label in enumerate(labels):\n","        x, y = low_dim_embs[i, :]\n","        plt.scatter(x, y)\n","        plt.annotate(label,\n","                     xy=(x, y),\n","                     xytext=(5, 2),\n","                     textcoords='offset points',\n","                     ha='right',\n","                     va='bottom')\n","    plt.show()\n","    if filename is not None:\n","        plt.savefig(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2500)\n","plot_only = 500 #only top 500 words\n","low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n","labels = [reverse_dictionary[i] for i in range(plot_only)]\n","plot_with_labels(low_dim_embs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tsne2 = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n","plot_only = 10000#only top 10000 words\n","low_dim_embs2 = tsne2.fit_transform(final_embeddings[:plot_only, :])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = [reverse_dictionary[i] for i in range(plot_only)]\n","word_list = ['กุมภาพันธ์', 'สิงหาคม',\n","             'และ', 'หรือ',\n","             'โรง', 'เรียน',\n","             'สงคราม', 'สหรัฐอเมริกา',\n","             'อังกฤษ', 'ญี่ปุ่น',\n","             'กิโลเมตร', 'เมตร',\n","             'ชาย', 'หญิง',\n","             'จำนวน', 'ทั้งหมด',\n","             'พระองค์', 'ท่าน',\n","             'ภาพ', 'เสียง']\n","idx_list = []\n","for word in word_list:\n","    idx_list.append(labels.index(word))\n","word_plot = low_dim_embs2[idx_list]\n","plot_with_labels(word_plot,word_list,filename=\"queen_king.png\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["From the above plot, we can observe the 10 semantic analogy examples of this data.  \n","Top-Bottom, Left-Right\n","|Word 1|Word 2|Reason|\n","|-|-|-|\n","|กิโลเมตร|เมตร|Because they are the types of length measurement in Thai (Kilometer & Meter).|\n","|กุมภาพันธ์|สิงหาคม|Because they are the month name of the year in Thai (Febuary & August).|\n","|พระองต์|ท่าน|Because พระองค์ can be considered as a way to call the royalty and ท่าน is also a way to call someone, so they are somewhat related to each other as a mean to call someone (His/Her Majesty & Sir).|\n","|อังกฤษ|ญี่ปุ่น|Because they are countries which had an alliance (Anglo-Japanese Alliance, พันธมิตรอังกฤษ-ญี่ปุ่น) with each other (England & Japan).|\n","|ชาย|หญิง|Because they are the biological genders (Male & Female).|\n","|สหรัฐอเมริกา|สงคราม|Because the United States of America (สหรัฐอเมริกา) often get involved in a war (สงคราม).|\n","|โรง|เรียน|Because these 2 words combined means School (โรงเรียน).|\n","|หรือ|และ|Because they are the conjunction of sentences (or & and).|\n","|จำนวน|ทั้งหมด|Because they are quantifiers (Amount & All/Total) and sometimes they are used together like จำนวนทั้งหมด (In total of...).|\n","|ภาพ|เสียง|Because they are the media used in communications (Picture/Image & Sound).|"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sLqG8WaNi-YE"},"source":["# Step 6: Extrinsic Evaluation\n","\n","## <font color='salmon'>Homework Question 5:</font>\n","<font color='salmon'>\n","Use the word embeddings from the skip-gram model as pre-trained weights (GloVe and fastText) in a classification model. Compare the result the with the same classification model that does not use the pre-trained weights. \n","</font>\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"dBPutcxEi-YF"},"outputs":[],"source":["all_news_filepath = glob.glob('data/BEST-TrainingSet/news/*.txt')\n","all_novel_filepath = glob.glob('data/BEST-TrainingSet/novel/*.txt')\n","all_article_filepath = glob.glob('data/BEST-TrainingSet/article/*.txt')\n","all_encyclopedia_filepath = glob.glob('data/BEST-TrainingSet/encyclopedia/*.txt')"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"ZaX-L5n4i-YG"},"outputs":[],"source":["from keras_preprocessing.sequence import pad_sequences\n","#preparing data for the classificaiton model\n","#In your homework, we will only use the first 2000 words in each text file\n","#any text file that has less than 2000 words will be padded\n","#reason:just to make this homework feasible under limited time and resource\n","max_length = 2000\n","def word_to_index(word):\n","    if word in dictionary:\n","        return dictionary[word]\n","    else:#if unknown\n","        return dictionary[\"UNK\"]\n","\n","\n","def prep_data():\n","    input_text = list()\n","    for textfile_path in [all_news_filepath, all_novel_filepath, all_article_filepath, all_encyclopedia_filepath]:\n","        for input_file in textfile_path:\n","            f = open(input_file,\"r\") #open file with name of \"*.txt\"\n","            text = re.sub(r'\\|', ' ', f.read()) # replace separation symbol with white space           \n","            text = re.sub(r'<\\W?\\w+>', '', text)# remove <NE> </NE> <AB> </AB> tags\n","            text = text.split() #split() method without an argument splits on whitespace \n","            indexed_text = list(map(lambda x:word_to_index(x), text[:max_length])) #map raw word string to its index   \n","            if 'news' in input_file:\n","                input_text.append([indexed_text,0]) \n","            elif 'novel' in input_file:\n","                input_text.append([indexed_text,1]) \n","            elif 'article' in input_file:\n","                input_text.append([indexed_text,2]) \n","            elif 'encyclopedia' in input_file:\n","                input_text.append([indexed_text,3]) \n","            \n","            f.close()\n","    random.shuffle(input_text)\n","    return input_text\n","\n","input_data = prep_data()\n","train_data = input_data[:int(len(input_data)*0.6)]\n","val_data = input_data[int(len(input_data)*0.6):int(len(input_data)*0.8)]\n","test_data = input_data[int(len(input_data)*0.8):]\n","\n","train_input = [data[0] for data in train_data]\n","train_input = pad_sequences(train_input, maxlen=max_length) #padding\n","train_target = [data[1] for data in train_data]\n","train_target=to_categorical(train_target, num_classes=4)\n","\n","val_input = [data[0] for data in val_data]\n","val_input = pad_sequences(val_input, maxlen=max_length) #padding\n","val_target = [data[1] for data in val_data]\n","val_target=to_categorical(val_target, num_classes=4)\n","\n","test_input = [data[0] for data in test_data]\n","test_input = pad_sequences(test_input, maxlen=max_length) #padding\n","test_target = [data[1] for data in test_data]\n","test_target=to_categorical(test_target, num_classes=4)\n","\n","del input_data, val_data,train_data, test_data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **Get embedding matrix**"]},{"cell_type":"markdown","metadata":{},"source":["#### **Get embedding matrix**"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def get_embeddings_matrix(embeddings,num_words,embedding_dim,dictionary, dict_name = \"embeddings\"):\n","    hits = 0\n","    misses = 0\n","    embedding_matrix = np.zeros((num_words, embedding_dim))\n","    for word, i in dictionary.items():\n","        embedding_vector = embeddings.get(word)\n","        if embedding_vector is not None:\n","            # Words not found in embedding index will be all-zeros.\n","            # This includes the representation for \"padding\" and \"OOV\"\n","            embedding_matrix[i] = embedding_vector\n","            hits += 1\n","        else:\n","            misses += 1\n","    print(f\"from dict {dict_name}, found embeddings for {hits} words ({hits/num_words*100:.2f}%) and {misses} ({misses/num_words*100:.2f}%) not found.\")\n","    return embedding_matrix\n","\n","num_words = len(dictionary) + 1"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["from dict word2vec, found embeddings for 295164 words (100.00%) and 0 (0.00%) not found.\n","Emedding matrix shape: (295165, 32)\n"]}],"source":["word2vec_embedding_matrix = get_embeddings_matrix(word2vec,num_words,32,dictionary, dict_name = \"word2vec\")\n","print(f'Emedding matrix shape: {word2vec_embedding_matrix.shape}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **PythaiNLP**"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from pythainlp import word_vector\n","\n","pythainlp_model = word_vector.WordVector(model_name=\"thai2fit_wv\").get_model() # load thai2fit_wv from pythainlp"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>290</th>\n","      <th>291</th>\n","      <th>292</th>\n","      <th>293</th>\n","      <th>294</th>\n","      <th>295</th>\n","      <th>296</th>\n","      <th>297</th>\n","      <th>298</th>\n","      <th>299</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ที่</th>\n","      <td>0.308956</td>\n","      <td>-0.097699</td>\n","      <td>0.116745</td>\n","      <td>0.215612</td>\n","      <td>0.015768</td>\n","      <td>-0.064163</td>\n","      <td>0.062168</td>\n","      <td>0.039649</td>\n","      <td>0.864940</td>\n","      <td>0.846904</td>\n","      <td>...</td>\n","      <td>-0.142418</td>\n","      <td>0.033241</td>\n","      <td>0.171581</td>\n","      <td>-0.624864</td>\n","      <td>-0.009358</td>\n","      <td>0.449131</td>\n","      <td>0.120130</td>\n","      <td>-0.122195</td>\n","      <td>-0.450617</td>\n","      <td>-0.071318</td>\n","    </tr>\n","    <tr>\n","      <th>และ</th>\n","      <td>0.010751</td>\n","      <td>-0.618971</td>\n","      <td>0.129665</td>\n","      <td>0.035460</td>\n","      <td>-0.007560</td>\n","      <td>0.027607</td>\n","      <td>0.397824</td>\n","      <td>0.026543</td>\n","      <td>0.254075</td>\n","      <td>0.168328</td>\n","      <td>...</td>\n","      <td>-0.105786</td>\n","      <td>0.180930</td>\n","      <td>-0.101630</td>\n","      <td>0.070885</td>\n","      <td>-0.037263</td>\n","      <td>0.183606</td>\n","      <td>-0.049088</td>\n","      <td>-0.672288</td>\n","      <td>-1.293044</td>\n","      <td>0.592576</td>\n","    </tr>\n","    <tr>\n","      <th>เป็น</th>\n","      <td>-0.015736</td>\n","      <td>-0.258926</td>\n","      <td>0.052953</td>\n","      <td>0.153728</td>\n","      <td>-0.005985</td>\n","      <td>-0.021081</td>\n","      <td>0.041088</td>\n","      <td>0.057312</td>\n","      <td>1.633230</td>\n","      <td>0.442729</td>\n","      <td>...</td>\n","      <td>-0.009408</td>\n","      <td>-0.252576</td>\n","      <td>-0.305512</td>\n","      <td>0.372542</td>\n","      <td>0.049151</td>\n","      <td>0.568470</td>\n","      <td>0.266586</td>\n","      <td>0.400800</td>\n","      <td>-0.784650</td>\n","      <td>0.197369</td>\n","    </tr>\n","    <tr>\n","      <th>ของ</th>\n","      <td>-0.189711</td>\n","      <td>-0.174774</td>\n","      <td>0.171124</td>\n","      <td>-0.186771</td>\n","      <td>0.054294</td>\n","      <td>-0.114150</td>\n","      <td>-1.109456</td>\n","      <td>-0.094466</td>\n","      <td>-0.447015</td>\n","      <td>0.042377</td>\n","      <td>...</td>\n","      <td>-0.168676</td>\n","      <td>-0.148738</td>\n","      <td>0.680404</td>\n","      <td>0.097702</td>\n","      <td>0.020270</td>\n","      <td>0.182967</td>\n","      <td>-0.083949</td>\n","      <td>0.006287</td>\n","      <td>-0.707434</td>\n","      <td>-0.070234</td>\n","    </tr>\n","    <tr>\n","      <th>มี</th>\n","      <td>-0.156962</td>\n","      <td>-0.231863</td>\n","      <td>0.080312</td>\n","      <td>0.323157</td>\n","      <td>0.215695</td>\n","      <td>0.055145</td>\n","      <td>0.420794</td>\n","      <td>0.016842</td>\n","      <td>0.256759</td>\n","      <td>0.832864</td>\n","      <td>...</td>\n","      <td>-0.044267</td>\n","      <td>-0.147186</td>\n","      <td>-0.105424</td>\n","      <td>0.907078</td>\n","      <td>0.009299</td>\n","      <td>0.550953</td>\n","      <td>0.139337</td>\n","      <td>0.031696</td>\n","      <td>-0.670379</td>\n","      <td>-0.008048</td>\n","    </tr>\n","    <tr>\n","      <th>ได้</th>\n","      <td>-0.428813</td>\n","      <td>-0.031194</td>\n","      <td>0.041922</td>\n","      <td>-0.036608</td>\n","      <td>-0.008106</td>\n","      <td>0.076470</td>\n","      <td>-0.782270</td>\n","      <td>0.033361</td>\n","      <td>0.606864</td>\n","      <td>0.440520</td>\n","      <td>...</td>\n","      <td>0.024458</td>\n","      <td>-0.025031</td>\n","      <td>0.103389</td>\n","      <td>-0.078255</td>\n","      <td>0.034323</td>\n","      <td>0.459774</td>\n","      <td>-0.748643</td>\n","      <td>0.337775</td>\n","      <td>-0.487408</td>\n","      <td>-0.511535</td>\n","    </tr>\n","    <tr>\n","      <th>\"\"\"\"</th>\n","      <td>-0.287710</td>\n","      <td>0.064193</td>\n","      <td>0.205076</td>\n","      <td>0.146356</td>\n","      <td>-0.071343</td>\n","      <td>-0.039451</td>\n","      <td>-1.845461</td>\n","      <td>0.163763</td>\n","      <td>1.018096</td>\n","      <td>0.272786</td>\n","      <td>...</td>\n","      <td>0.051024</td>\n","      <td>-0.532856</td>\n","      <td>-0.131856</td>\n","      <td>-0.090323</td>\n","      <td>-0.058895</td>\n","      <td>0.151262</td>\n","      <td>-0.420358</td>\n","      <td>0.055971</td>\n","      <td>-0.930814</td>\n","      <td>0.163908</td>\n","    </tr>\n","    <tr>\n","      <th>การ</th>\n","      <td>0.239587</td>\n","      <td>-0.303620</td>\n","      <td>0.079953</td>\n","      <td>-0.453045</td>\n","      <td>-0.528826</td>\n","      <td>-0.161692</td>\n","      <td>0.235725</td>\n","      <td>-0.099673</td>\n","      <td>0.691668</td>\n","      <td>0.536159</td>\n","      <td>...</td>\n","      <td>-0.110436</td>\n","      <td>-0.297495</td>\n","      <td>-0.217414</td>\n","      <td>0.045158</td>\n","      <td>0.066647</td>\n","      <td>0.190095</td>\n","      <td>-0.304333</td>\n","      <td>-0.724927</td>\n","      <td>-0.995488</td>\n","      <td>-0.716609</td>\n","    </tr>\n","    <tr>\n","      <th>(</th>\n","      <td>-0.120522</td>\n","      <td>-0.355783</td>\n","      <td>0.168180</td>\n","      <td>-0.377733</td>\n","      <td>-0.158624</td>\n","      <td>-0.047249</td>\n","      <td>0.361140</td>\n","      <td>0.161460</td>\n","      <td>0.913314</td>\n","      <td>0.345037</td>\n","      <td>...</td>\n","      <td>0.116285</td>\n","      <td>-0.318218</td>\n","      <td>-0.356664</td>\n","      <td>0.519889</td>\n","      <td>0.130475</td>\n","      <td>0.125772</td>\n","      <td>0.101328</td>\n","      <td>-0.382658</td>\n","      <td>-1.205359</td>\n","      <td>0.340139</td>\n","    </tr>\n","    <tr>\n","      <th>)</th>\n","      <td>-0.086848</td>\n","      <td>-0.155231</td>\n","      <td>0.133015</td>\n","      <td>-0.039913</td>\n","      <td>0.183761</td>\n","      <td>0.115142</td>\n","      <td>-1.940854</td>\n","      <td>-0.066565</td>\n","      <td>-2.399744</td>\n","      <td>0.146722</td>\n","      <td>...</td>\n","      <td>0.019406</td>\n","      <td>-0.181474</td>\n","      <td>0.099863</td>\n","      <td>0.516092</td>\n","      <td>0.201697</td>\n","      <td>0.249139</td>\n","      <td>0.252957</td>\n","      <td>1.138815</td>\n","      <td>-0.018209</td>\n","      <td>0.232265</td>\n","    </tr>\n","    <tr>\n","      <th>โดย</th>\n","      <td>0.022566</td>\n","      <td>-0.456885</td>\n","      <td>0.139862</td>\n","      <td>-0.094409</td>\n","      <td>-0.148019</td>\n","      <td>-0.015714</td>\n","      <td>0.217329</td>\n","      <td>-0.237290</td>\n","      <td>1.615968</td>\n","      <td>0.389083</td>\n","      <td>...</td>\n","      <td>-0.020279</td>\n","      <td>0.255168</td>\n","      <td>0.638628</td>\n","      <td>0.425027</td>\n","      <td>0.113261</td>\n","      <td>0.287361</td>\n","      <td>-0.088324</td>\n","      <td>-0.497313</td>\n","      <td>-0.374997</td>\n","      <td>0.178059</td>\n","    </tr>\n","    <tr>\n","      <th>ส</th>\n","      <td>0.017510</td>\n","      <td>-0.218752</td>\n","      <td>0.096772</td>\n","      <td>-0.292116</td>\n","      <td>0.574779</td>\n","      <td>0.504320</td>\n","      <td>0.366973</td>\n","      <td>0.175230</td>\n","      <td>0.193527</td>\n","      <td>0.151991</td>\n","      <td>...</td>\n","      <td>-0.184546</td>\n","      <td>-0.585278</td>\n","      <td>-0.224206</td>\n","      <td>0.185302</td>\n","      <td>0.339830</td>\n","      <td>0.003586</td>\n","      <td>-0.008413</td>\n","      <td>-0.125818</td>\n","      <td>-0.188513</td>\n","      <td>0.042182</td>\n","    </tr>\n","    <tr>\n","      <th>น</th>\n","      <td>-0.650245</td>\n","      <td>0.606879</td>\n","      <td>0.072588</td>\n","      <td>0.265695</td>\n","      <td>-0.059699</td>\n","      <td>0.210481</td>\n","      <td>1.053988</td>\n","      <td>0.210932</td>\n","      <td>0.184617</td>\n","      <td>0.964869</td>\n","      <td>...</td>\n","      <td>-0.372368</td>\n","      <td>-0.086402</td>\n","      <td>0.878128</td>\n","      <td>-0.223392</td>\n","      <td>0.100727</td>\n","      <td>0.017854</td>\n","      <td>0.177820</td>\n","      <td>-0.335889</td>\n","      <td>-0.213907</td>\n","      <td>-0.564454</td>\n","    </tr>\n","    <tr>\n","      <th>กับ</th>\n","      <td>-0.040384</td>\n","      <td>-0.421850</td>\n","      <td>0.126059</td>\n","      <td>0.146927</td>\n","      <td>-0.000009</td>\n","      <td>-0.089853</td>\n","      <td>-0.713599</td>\n","      <td>0.018187</td>\n","      <td>0.322403</td>\n","      <td>0.541420</td>\n","      <td>...</td>\n","      <td>-0.187506</td>\n","      <td>-0.388785</td>\n","      <td>0.104563</td>\n","      <td>0.096133</td>\n","      <td>-0.025143</td>\n","      <td>0.550998</td>\n","      <td>-0.043870</td>\n","      <td>-0.257404</td>\n","      <td>-0.885548</td>\n","      <td>-0.246913</td>\n","    </tr>\n","    <tr>\n","      <th>จะ</th>\n","      <td>-0.547274</td>\n","      <td>-0.275737</td>\n","      <td>0.124294</td>\n","      <td>0.365538</td>\n","      <td>0.180782</td>\n","      <td>-0.061260</td>\n","      <td>-0.270269</td>\n","      <td>0.025696</td>\n","      <td>0.588213</td>\n","      <td>0.166396</td>\n","      <td>...</td>\n","      <td>-0.055187</td>\n","      <td>0.132841</td>\n","      <td>0.390810</td>\n","      <td>-0.140955</td>\n","      <td>0.188449</td>\n","      <td>0.429394</td>\n","      <td>-0.502668</td>\n","      <td>1.109359</td>\n","      <td>-0.733541</td>\n","      <td>-1.343620</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15 rows × 300 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6    \\\n","ที่   0.308956 -0.097699  0.116745  0.215612  0.015768 -0.064163  0.062168   \n","และ   0.010751 -0.618971  0.129665  0.035460 -0.007560  0.027607  0.397824   \n","เป็น -0.015736 -0.258926  0.052953  0.153728 -0.005985 -0.021081  0.041088   \n","ของ  -0.189711 -0.174774  0.171124 -0.186771  0.054294 -0.114150 -1.109456   \n","มี   -0.156962 -0.231863  0.080312  0.323157  0.215695  0.055145  0.420794   \n","ได้  -0.428813 -0.031194  0.041922 -0.036608 -0.008106  0.076470 -0.782270   \n","\"\"\"\" -0.287710  0.064193  0.205076  0.146356 -0.071343 -0.039451 -1.845461   \n","การ   0.239587 -0.303620  0.079953 -0.453045 -0.528826 -0.161692  0.235725   \n","(    -0.120522 -0.355783  0.168180 -0.377733 -0.158624 -0.047249  0.361140   \n",")    -0.086848 -0.155231  0.133015 -0.039913  0.183761  0.115142 -1.940854   \n","โดย   0.022566 -0.456885  0.139862 -0.094409 -0.148019 -0.015714  0.217329   \n","ส     0.017510 -0.218752  0.096772 -0.292116  0.574779  0.504320  0.366973   \n","น    -0.650245  0.606879  0.072588  0.265695 -0.059699  0.210481  1.053988   \n","กับ  -0.040384 -0.421850  0.126059  0.146927 -0.000009 -0.089853 -0.713599   \n","จะ   -0.547274 -0.275737  0.124294  0.365538  0.180782 -0.061260 -0.270269   \n","\n","           7         8         9    ...       290       291       292  \\\n","ที่   0.039649  0.864940  0.846904  ... -0.142418  0.033241  0.171581   \n","และ   0.026543  0.254075  0.168328  ... -0.105786  0.180930 -0.101630   \n","เป็น  0.057312  1.633230  0.442729  ... -0.009408 -0.252576 -0.305512   \n","ของ  -0.094466 -0.447015  0.042377  ... -0.168676 -0.148738  0.680404   \n","มี    0.016842  0.256759  0.832864  ... -0.044267 -0.147186 -0.105424   \n","ได้   0.033361  0.606864  0.440520  ...  0.024458 -0.025031  0.103389   \n","\"\"\"\"  0.163763  1.018096  0.272786  ...  0.051024 -0.532856 -0.131856   \n","การ  -0.099673  0.691668  0.536159  ... -0.110436 -0.297495 -0.217414   \n","(     0.161460  0.913314  0.345037  ...  0.116285 -0.318218 -0.356664   \n",")    -0.066565 -2.399744  0.146722  ...  0.019406 -0.181474  0.099863   \n","โดย  -0.237290  1.615968  0.389083  ... -0.020279  0.255168  0.638628   \n","ส     0.175230  0.193527  0.151991  ... -0.184546 -0.585278 -0.224206   \n","น     0.210932  0.184617  0.964869  ... -0.372368 -0.086402  0.878128   \n","กับ   0.018187  0.322403  0.541420  ... -0.187506 -0.388785  0.104563   \n","จะ    0.025696  0.588213  0.166396  ... -0.055187  0.132841  0.390810   \n","\n","           293       294       295       296       297       298       299  \n","ที่  -0.624864 -0.009358  0.449131  0.120130 -0.122195 -0.450617 -0.071318  \n","และ   0.070885 -0.037263  0.183606 -0.049088 -0.672288 -1.293044  0.592576  \n","เป็น  0.372542  0.049151  0.568470  0.266586  0.400800 -0.784650  0.197369  \n","ของ   0.097702  0.020270  0.182967 -0.083949  0.006287 -0.707434 -0.070234  \n","มี    0.907078  0.009299  0.550953  0.139337  0.031696 -0.670379 -0.008048  \n","ได้  -0.078255  0.034323  0.459774 -0.748643  0.337775 -0.487408 -0.511535  \n","\"\"\"\" -0.090323 -0.058895  0.151262 -0.420358  0.055971 -0.930814  0.163908  \n","การ   0.045158  0.066647  0.190095 -0.304333 -0.724927 -0.995488 -0.716609  \n","(     0.519889  0.130475  0.125772  0.101328 -0.382658 -1.205359  0.340139  \n",")     0.516092  0.201697  0.249139  0.252957  1.138815 -0.018209  0.232265  \n","โดย   0.425027  0.113261  0.287361 -0.088324 -0.497313 -0.374997  0.178059  \n","ส     0.185302  0.339830  0.003586 -0.008413 -0.125818 -0.188513  0.042182  \n","น    -0.223392  0.100727  0.017854  0.177820 -0.335889 -0.213907 -0.564454  \n","กับ   0.096133 -0.025143  0.550998 -0.043870 -0.257404 -0.885548 -0.246913  \n","จะ   -0.140955  0.188449  0.429394 -0.502668  1.109359 -0.733541 -1.343620  \n","\n","[15 rows x 300 columns]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["#create dataframe\n","thai2dict = {}\n","for word in pythainlp_model.index_to_key:\n","    thai2dict[word] = pythainlp_model[word]\n","thai2vec = pd.DataFrame.from_dict(thai2dict,orient='index')\n","thai2vec.head(15)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["from dict thai2dict, found embeddings for 26099 words (8.84%) and 269065 (91.16%) not found.\n"]}],"source":["thai2dict_embeddings = get_embeddings_matrix(thai2dict, num_words, 300, dictionary, dict_name = \"thai2dict\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Missing words in the dictionary: 269065\n"]}],"source":["# Missing words in the dictionary\n","missing_words = []\n","for word, i in dictionary.items():\n","    if word not in thai2dict:\n","        missing_words.append(word)\n","print(f\"Missing words in the dictionary: {len(missing_words)}\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["['for_keras_zero_padding',\n"," 'ใน',\n"," '\"',\n"," 'เรียกว่า',\n"," 'ของการ',\n"," 'ฯ',\n"," '/',\n"," 'ส์',\n"," 'The',\n"," 'หตุการณ์',\n"," 'เกิล',\n"," '(พ.ศ.',\n"," 'ฉียง',\n"," 'ซิง',\n"," 'ท์',\n"," ',000',\n"," 'ด้านการ',\n"," 'คริสต์ศตวรรษ',\n"," 'งานที่',\n"," '(ค.ศ.',\n"," 'A',\n"," 'เรีย',\n"," 'สเปน',\n"," 'เป็นอันดับ',\n"," 'นั',\n"," 'สำเร็จการ',\n"," 'เรียนการ',\n"," '<br>',\n"," 'บริษั',\n"," 'ขึ้นที่',\n"," 'สดุ',\n"," 'formula_',\n"," 'จุฬาลงกรณ์มหาวิทยาลัย',\n"," 'ดียว',\n"," 'กองทั',\n"," 'สมเด็จพระเจ้า',\n"," 'พระโอรส',\n"," 'สโมสรฟุตบอล',\n"," 'มหาวิหาร',\n"," 'B',\n"," 'ประเทศฝรั่งเศส',\n"," 'ดอลลา',\n"," 'C',\n"," 'ผ่านการ',\n"," 'ดูกาล',\n"," 'I',\n"," 'กรุงเทพ',\n"," 'พระกรุณา',\n"," 'มิวสิก',\n"," 'เปน']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["missing_words[:50]"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","word: พระกรุณา with index 1413 \n","word: พระกรุณา is not in pythainlp: False\n"]}],"source":["w = 'พระกรุณา'\n","print(True if w in tokens else False)\n","print(f'word: {w} with index {dictionary[w]} 'if w in dictionary else f'word: {w} is not in dictionary: {w in dictionary}')\n","print(f'word: {w} in pythainlp: {w in pythainlp_model.index_to_key} with value {pythainlp_model[w]}' if w in pythainlp_model.index_to_key else f'word: {w} is not in pythainlp: {w in pythainlp_model.index_to_key}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **FastText**"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]}],"source":["import fasttext\n","\n","path_to_fasttext = 'embedded/cc.en.300.bin'\n","fasttext_model = fasttext.load_model(path_to_fasttext)\n","fasttext_model.get_dimension()"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["from dict fasttext, found embeddings for 51196 words (17.34%) and 243968 (82.65%) not found.\n"]}],"source":["fasttext_dict = {}\n","for word in fasttext_model.get_words():\n","    fasttext_dict[word] = fasttext_model.get_word_vector(word)\n","    \n","fasttext_embeddings = get_embeddings_matrix(fasttext_dict, num_words, 300, dictionary, dict_name = \"fasttext\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **GloVe** (Not Used)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# path_to_glove_file = \"embedded/glove.42B.300d.txt\"\n","\n","# embeddings_index = {}\n","# with open(path_to_glove_file) as f:\n","#     for line in f:\n","#         word, coefs = line.split(maxsplit=1)\n","#         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","#         embeddings_index[word] = coefs\n","        \n","# print(f'Found {len(embeddings_index)} word vectors. with dimension {len(embeddings_index[\"the\"])}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **Build the model**"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["#the classification model\n","#TODO#5 find out how to initialize your embedding layer with pre-trained weights, evaluate and observe\n","#don't forget to compare it with the same model that does not use pre-trained weights\n","#you can use your own model too! and feel free to customize this model as you wish\n","# more information --> https://keras.io/examples/nlp/pretrained_word_embeddings/\n","# fastText --> https://fasttext.cc/docs/en/crawl-vectors.html (optional)\n","# !wget --no-check-certificate https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n","\n","# cls_model = Sequential()\n","# cls_model.add(Embedding(len(dictionary)+1, 32, input_length=max_length,mask_zero=True)) \n","# cls_model.add(GRU(32))\n","# cls_model.add(Dropout(0.5))\n","# cls_model.add(Dense(4, activation='softmax'))\n","# opt=Adam(lr=0.01)\n","# cls_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","# cls_model.summary()\n","# print('Train...')\n","# cls_model.fit(train_input, train_target,\n","#           epochs=10,\n","#           validation_data=[val_input, val_target])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from time import time\n","from datetime import timedelta\n","\n","def build_model(model, address = None,X = None, Y = None, x_val = None, y_val = None, batch_size = 32, epochs = 10):\n","    \"\"\"\n","    Fit the model if the model checkpoint does not exist or else\n","    load it from that address.\n","    \"\"\"\n","    if (not os.path.exists(address)):\n","        print(f'Model checkpoint does not exist. Building model and saving it to {address}...')\n","        stop = EarlyStopping(monitor = 'val_loss', min_delta = 0, \n","                             patience = 5, verbose = 1, mode = 'auto')\n","        save = ModelCheckpoint(address, monitor = 'val_loss', \n","                               verbose = 0, save_best_only = True)\n","        callbacks = [stop, save]\n","\n","        start = time()\n","        history = model.fit(X, Y, batch_size = batch_size, \n","                            epochs = epochs, verbose = 1,\n","                            validation_data = (x_val, y_val),\n","                            callbacks = callbacks)\n","        elapse = time() - start\n","        print('elapsed time: ', elapse)\n","        model_info = {'history': history, 'elapse': elapse, 'model': model}\n","        model.save(address)\n","    else:\n","        print(f'Model checkpoint exists. Loading model from {address}...')\n","        model = load_model(address)\n","        model_info = {'model': model}\n","\n","    return model_info"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def cls_model_with_pretrained_weights(name=\"cls_model\",embedding_dim=32,embedding_matrix=None,trainable=False):\n","    input = Input(shape=(max_length,))\n","    if embedding_matrix is not None:\n","        embedding = Embedding(len(dictionary)+1,\n","                              embedding_dim,\n","                              input_length=len(dictionary)+1,\n","                              mask_zero=True,\n","                              weights=[embedding_matrix],\n","                              trainable=trainable)(input)\n","    else:\n","        embedding = Embedding(len(dictionary)+1,\n","                              embedding_dim,\n","                              input_length=len(dictionary)+1,\n","                              mask_zero=trainable)(input)\n","    x = GRU(32)(embedding)\n","    x = Dropout(0.5)(x)\n","    output = Dense(4, activation='softmax')(x)\n","    model = Model(inputs=input, outputs=output,name=name)\n","    opt=Adam(learning_rate=0.01)\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    model.summary()\n","    return model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 1: Without Pre-trained weights**"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"cls_model_nopretrain\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 2000)]            0         \n","                                                                 \n"," embedding_2 (Embedding)     (None, 2000, 32)          9445280   \n","                                                                 \n"," gru (GRU)                   (None, 32)                6336      \n","                                                                 \n"," dropout (Dropout)           (None, 32)                0         \n","                                                                 \n"," dense (Dense)               (None, 4)                 132       \n","                                                                 \n","=================================================================\n","Total params: 9,451,748\n","Trainable params: 9,451,748\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["cls_model_nopretrain = cls_model_with_pretrained_weights(name=\"cls_model_nopretrain\",embedding_matrix=None)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 2: With Pre-trained weights skip-gram**"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"cls_modle_skip\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 2000)]            0         \n","                                                                 \n"," embedding_3 (Embedding)     (None, 2000, 32)          9445280   \n","                                                                 \n"," gru_1 (GRU)                 (None, 32)                6336      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 132       \n","                                                                 \n","=================================================================\n","Total params: 9,451,748\n","Trainable params: 9,451,748\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["cls_modle_skip = cls_model_with_pretrained_weights(name=\"cls_modle_skip\",embedding_matrix=word2vec_embedding_matrix,trainable=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 3: With Pre-trained weights pythainlp**"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"cls_model_pythainlp\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_5 (InputLayer)        [(None, 2000)]            0         \n","                                                                 \n"," embedding_4 (Embedding)     (None, 2000, 300)         88549500  \n","                                                                 \n"," gru_2 (GRU)                 (None, 32)                32064     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 4)                 132       \n","                                                                 \n","=================================================================\n","Total params: 88,581,696\n","Trainable params: 88,581,696\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["cls_model_pythainlp = cls_model_with_pretrained_weights(name=\"cls_model_pythainlp\",embedding_dim=300,embedding_matrix=thai2dict_embeddings,trainable=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 4: With Pre-trained weights fasttext**"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"cls_model_fasttext\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_6 (InputLayer)        [(None, 2000)]            0         \n","                                                                 \n"," embedding_5 (Embedding)     (None, 2000, 300)         88549500  \n","                                                                 \n"," gru_3 (GRU)                 (None, 32)                32064     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 4)                 132       \n","                                                                 \n","=================================================================\n","Total params: 88,581,696\n","Trainable params: 88,581,696\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["cls_model_fasttext = cls_model_with_pretrained_weights(name=\"cls_model_fasttext\",embedding_dim=300,embedding_matrix=fasttext_embeddings,trainable=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **Train the model**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 1: Without Pre-trained weights**"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model checkpoint exists. Loading model from cls_model_nopretrain...\n"]}],"source":["cls_model_nopretrain_info = build_model(cls_model_nopretrain,\n","                                        address = 'cls_model_nopretrain',\n","                                        X = train_input, Y = train_target,\n","                                        x_val = val_input, y_val = val_target,\n","                                        batch_size = 32, epochs = 10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 2: With Pre-trained weights skip-gram**"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model checkpoint exists. Loading model from cls_modle_skip...\n"]}],"source":["cls_modle_skip_info = build_model(cls_modle_skip,\n","                                  address = 'cls_modle_skip',\n","                                  X = train_input, Y = train_target,\n","                                  x_val = val_input, y_val = val_target,\n","                                  batch_size = 32, epochs = 10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 3: With Pre-trained weights pythaiNLP**"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model checkpoint exists. Loading model from cls_model_pythainlp...\n"]}],"source":["cls_model_pythainlp_info = build_model(cls_model_pythainlp,\n","                                       address = 'cls_model_pythainlp',\n","                                       X = train_input, Y = train_target,\n","                                       x_val = val_input, y_val = val_target,\n","                                       batch_size = 32, epochs = 10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Model 4: With Pre-trained weights fasttext**"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model checkpoint exists. Loading model from cls_model_fasttext...\n"]}],"source":["cls_model_fasttext_info = build_model(cls_model_fasttext,\n","                                      address = 'cls_model_fasttext',\n","                                      X = train_input, Y = train_target,\n","                                      x_val = val_input, y_val = val_target,\n","                                      batch_size = 32, epochs = 10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### **Evaluate the model**"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4/4 [==============================] - 0s 33ms/step - loss: 2.2291 - accuracy: 0.4216\n","4/4 [==============================] - 3s 434ms/step - loss: 2.9985 - accuracy: 0.4412\n","4/4 [==============================] - 3s 471ms/step - loss: 1.3140 - accuracy: 0.5980\n","4/4 [==============================] - 3s 523ms/step - loss: 1.3528 - accuracy: 0.6961\n"]}],"source":["results_nopretrain = cls_model_nopretrain_info['model'].evaluate(test_input, test_target, batch_size=32)\n","results_fasttext = cls_model_fasttext_info['model'].evaluate(test_input, test_target, batch_size=32)\n","results_skip = cls_modle_skip_info['model'].evaluate(test_input, test_target, batch_size=32)\n","results_pythainlp = cls_model_pythainlp_info['model'].evaluate(test_input, test_target, batch_size=32)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No pretrain model loss: 2.2290711402893066 accuracy: 0.4215686321258545\n","Fasttext model loss: 2.9984896183013916 accuracy: 0.44117647409439087\n","Skip model loss: 1.3140294551849365 accuracy: 0.5980392098426819\n","Pythainlp model loss: 1.352795958518982 accuracy: 0.6960784196853638\n"]}],"source":["print(f'No pretrain model loss: {results_nopretrain[0]} accuracy: {results_nopretrain[1]}')\n","print(f'Fasttext model loss: {results_fasttext[0]} accuracy: {results_fasttext[1]}')\n","print(f'Skip model loss: {results_skip[0]} accuracy: {results_skip[1]}')\n","print(f'Pythainlp model loss: {results_pythainlp[0]} accuracy: {results_pythainlp[1]}')\n","\n","# results = cls_modle_skip.evaluate(test_input, test_target)\n","# print(\"test loss, test acc:\", results)"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"99ba4c920f2b55ec7afa7ece9415a75624cf997d3db7f5cda94c448a08d2153a"}}},"nbformat":4,"nbformat_minor":0}
