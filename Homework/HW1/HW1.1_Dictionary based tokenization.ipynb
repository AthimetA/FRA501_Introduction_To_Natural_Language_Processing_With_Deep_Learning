{"cells":[{"cell_type":"markdown","metadata":{"id":"7EdbVSmE87En"},"source":["# HW1: Dictionary-based Tokenization \n"]},{"cell_type":"markdown","metadata":{"id":"pJJLm1Ub87Et"},"source":["In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete: \n","<br>\n","* maximal_matching\n","* backtrack\n","</br>\n","\n","Also, you have to find how to use word_tokenize() in PythaiNLP along with customer_dict by yourselves."]},{"cell_type":"markdown","metadata":{"id":"DF5Pme7CK3YF"},"source":["## Part1) Your Maximal Matching with Your Dictionary"]},{"cell_type":"markdown","metadata":{"id":"xzs0R06q87Et"},"source":["### Create a toy dictionary to test the algorithm\n","\n","This is based on the example shown in the lecture. \n","You will tokenize the following text string: \"ไปหามเหสี!\"\n","The toy dictoionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"pq3W4p3z87Ev"},"outputs":[],"source":["thai_vocab = [\"ไ\",\"ป\",\"ห\",\"า\",\"ม\",\"เ\",\"ห\",\"ส\",\"ี\",\"ไป\",\"หา\",\"หาม\",\"เห\",\"สี\",\"มเหสี\",\"!\"]"]},{"cell_type":"markdown","metadata":{"id":"ZornooGF87Ew"},"source":["### Maximal matching \n","Complete the maximal matching  function below to tokenize the input text\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Ao4d2E3387Ew"},"outputs":[],"source":["# c is the character array\n","from math import inf #infinity\n","def maximal_matching(c, verbose=False):\n","    # Define a function to print if verbose is True for debugging\n","    verboseprint = print if verbose else lambda *a, **k: None\n","    #Initialize an empty 2D list\n","    d  =[[None]*len(c) for _ in range(len(c))]\n","    ####FILL CODE HERE####\n","    for i in range(len(c)):\n","        verboseprint('-'*20)\n","        verboseprint('i =', i)\n","        for j in range(i, len(c)):\n","            verboseprint('Char: \"'+c[i:j+1]+'\", j =', j)\n","            if c[i:j+1] in thai_vocab and i == 0:\n","                verboseprint('first condition')\n","                d[i][j] = 1\n","            elif c[i:j+1] in thai_vocab:\n","                verboseprint('second condition')\n","                d[i][j] = 1 + min(d[k][i-1] for k in range(i))\n","            else:\n","                verboseprint('third condition')\n","                d[i][j] = inf\n","        verboseprint(d[i])\n","    ######################\n","    return d\n","\n","# input_text = \"ไปหามเหสี!\"\n","# out = maximal_matching(input_text, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"w7vBXfjM87Ew"},"source":["### Backtracking\n","Complete the backtracking function below to find the tokenzied words.\n","It should return a list containing a pair of the beginning position and the ending position of each word.\n","In this example, it should return: \n","<br>\n","[(0, 1),(2, 3),(4, 8),(9, 9)]\n","<br> \n","#### Each pair contains the position of each word as follows:\n","(0, 1) ไป\n","<br>\n","(2, 3) หา\n","<br>\n","(4, 8) มเหสี\n","<br>\n","(9, 9) !\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"SxNFf1IE87Ex"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------\n","eow = 8\n","d[8][8] = 5\n","d[7][8] = 4\n","d[4][8] = 3\n","for column 8 the min is 3 at row 4\n","--------------------\n","eow = 3\n","d[3][3] = 3\n","d[2][3] = 2\n","for column 3 the min is 2 at row 2\n","--------------------\n","eow = 1\n","d[1][1] = 2\n","d[0][1] = 1\n","for column 1 the min is 1 at row 0\n","[(0, 1), (2, 3), (4, 8)]\n"]}],"source":["def backtrack(d, verbose=False):\n","    # Define a function to print if verbose is True for debugging\n","    verboseprint = print if verbose else lambda *a, **k: None\n","    eow = len(d)-1 # End of Word position\n","    word_pos = [] # Word position\n","    ####FILL CODE HERE####\n","    while eow >= 0:\n","        verboseprint('-'*20)\n","        verboseprint('eow =', eow)\n","        min = inf\n","        for i in range(eow, -1, -1):\n","            if d[i][eow] < min:\n","                verboseprint('d[{}][{}] = {}'.format(i, eow, d[i][eow]))\n","                min = d[i][eow]\n","                min_i = i\n","        verboseprint('for column {} the min is {} at row {}'.format(eow, min, min_i))\n","        word_pos.append((min_i, eow))\n","        eow = min_i-1\n","    ######################\n","    word_pos.reverse()\n","    return word_pos\n","\n","input_text = \"ไปหามเหสี\"\n","out = maximal_matching(input_text)\n","word_pos = backtrack(out, verbose=True)\n","print(word_pos)"]},{"cell_type":"markdown","metadata":{"id":"q0MJkKsh87Ex"},"source":["### Test your maximal matching algorithm on a toy dictionary\n","\n","Expected output:\n","\n","[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n","<br>\n","[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","<br>\n","[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n","<br>\n","[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n","<br>\n","[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n","<br>\n","[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n","<br>\n","[None, None, None, None, None, None, 4, inf, inf, inf] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4, inf] ส\n","<br>\n","[None, None, None, None, None, None, None, None, 5, inf] ี\n","<br>\n","[None, None, None, None, None, None, None, None, None, 4] !\n","<br>"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"tsmVQIKS87Ey"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n","[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n","[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n","[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n","[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n","[None, None, None, None, None, None, 4, inf, inf, inf] ห\n","[None, None, None, None, None, None, None, 4, 4, inf] ส\n","[None, None, None, None, None, None, None, None, 5, inf] ี\n","[None, None, None, None, None, None, None, None, None, 4] !\n"]}],"source":["input_text = \"ไปหามเหสี!\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"]},{"cell_type":"markdown","metadata":{"id":"IVhCMM4d87Ey"},"source":["### Test your backtracking algorithm on a toy dictionary\n","Expected output:\n","<br>\n","ไป|หา|มเหสี|!"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6Hurbm1f87Ey"},"outputs":[{"name":"stdout","output_type":"stream","text":["ไป|หา|มเหสี|!\n"]}],"source":["def print_tokenized_text(d, input_text):\n","    tokenized_text=[]\n","    for pos in backtrack(d):\n","        #print(pos)\n","        tokenized_text.append(input_text[pos[0]:pos[1]+1])\n","\n","    print(\"|\".join(tokenized_text))\n","    \n","print_tokenized_text(out,input_text)"]},{"cell_type":"markdown","metadata":{"id":"57rP9cTU87Ez"},"source":["## Part2) Your Maximal Matching with Real Dictionary"]},{"cell_type":"markdown","metadata":{"id":"V306h7AG87Ez"},"source":["For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["62069\n","['ก ข ไม่กระดิกหู', 'ก.', 'ก.ค.', 'ก.ต.', 'ก.ป.ส.', 'ก.พ.', 'ก.พ.ด.', 'ก.ม.', 'ก.ย.', 'ก.ย']\n"]}],"source":["import urllib\n","\n","url = \"https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt\"\n","req = urllib.request.Request(url)\n","thai_vocab = []\n","\n","text = urllib.request.urlopen(req).read().decode('utf-8')\n","for line in text.splitlines():\n","    thai_vocab.append(line)\n","\n","# with open(\"words_th.txt\",encoding='utf-8-sig') as f:\n","#     thai_vocab = f.read().splitlines() \n","# print(\"Vocab size:\", len(thai_vocab))\n","# thai_vocab.extend([\"ๆ\",\"!\"])    \n","    \n","print(len(thai_vocab))\n","print(thai_vocab[:10])"]},{"cell_type":"markdown","metadata":{"id":"Kpjwzw1w87E0"},"source":["### The output of your maximal matching algoithm on a new dictionary\n","Expected output:\n","<br>\n","[1, 1, 100000, 1, 100000, 100000, 100000, 100000, 100000] ไ\n","<br>\n","[None, 2, 100000, 100000, 100000, 100000, 100000, 100000, 100000] ป\n","<br>\n","[None, None, 2, 2, 2, 100000, 100000, 100000, 100000] ห\n","<br>\n","[None, None, None, 100000, 100000, 100000, 100000, 100000, 100000] า\n","<br>\n","[None, None, None, None, 2, 100000, 100000, 100000, 2] ม\n","<br>\n","[None, None, None, None, None, 100000, 3, 100000, 100000] เ\n","<br>\n","[None, None, None, None, None, None, 100001, 100000, 100000] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4] ส\n","<br>\n","[None, None, None, None, None, None, None, None, None] ี"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"lYD5ChIS87E0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[inf, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n","[None, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","[None, None, inf, 2, 2, inf, inf, inf, inf] ห\n","[None, None, None, inf, inf, inf, inf, inf, inf] า\n","[None, None, None, None, inf, inf, inf, inf, 2] ม\n","[None, None, None, None, None, inf, 3, inf, inf] เ\n","[None, None, None, None, None, None, inf, inf, inf] ห\n","[None, None, None, None, None, None, None, inf, 4] ส\n","[None, None, None, None, None, None, None, None, inf] ี\n"]}],"source":["input_text = \"ไปหามเหสี\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"]},{"cell_type":"markdown","metadata":{"id":"BSqLuK7G87E0"},"source":["### Expected tokenized text\n","ไปหา|มเหสี"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"TI077jmy87E0"},"outputs":[{"name":"stdout","output_type":"stream","text":["ไปหา|มเหสี\n"]}],"source":["print_tokenized_text(out,input_text)"]},{"cell_type":"markdown","metadata":{"id":"VLGgO8PrLSz6"},"source":["## Part3) Maximal Matching from PythaiNLP"]},{"cell_type":"markdown","metadata":{"id":"LrZrzQoXLeUX"},"source":["### Default dictionary\n","\n","Study word_tokenize() from PythaiNLP in the link below.\n","\n","https://thainlp.org/pythainlp/docs/2.0/api/tokenize.html"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"yXxPBOcNLXfm"},"outputs":[],"source":["# !pip install pythainlp\n","# !pip install marisa_trie"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"goQE5gFUL4KO"},"outputs":[{"name":"stdout","output_type":"stream","text":["['นัด', 'กินกัน', 'ตอน', 'ไหน', 'ก็', 'ได้ที่', 'สามย่าน', 'มิตร', 'ทาวน์']\n"]}],"source":["from pythainlp.tokenize import word_tokenize\n","text='นัดกินกันตอนไหนก็ได้ที่สามย่านมิตรทาวน์'\n","\n","####FILL CODE HERE####\n","txt_tokenized = word_tokenize(text, engine='newmm', keep_whitespace=True)\n","print(txt_tokenized)\n","######################"]},{"cell_type":"markdown","metadata":{"id":"2SlX5cEBMHPd"},"source":["### Custom dictionary\n","\n","Add 'สามย่านมิตรทาวน์' into dictionary and then tokenize again"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"b4V9TqFaMPAj"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'marisa_trie.Trie'>\n","custom dictionary with marisa_trie: ['นัด', 'กินกัน', 'ตอน', 'ไหน', 'ก็', 'ได้ที่', 'สามย่านมิตรทาวน์']\n","custom dictionary with Tokenizer: ['นัด', 'กินกัน', 'ตอน', 'ไหน', 'ก็', 'ได้ที่', 'สามย่านมิตรทาวน์']\n"]}],"source":["####FILL CODE HERE####\n","import marisa_trie\n","\n","# Using marisa_trie to create a custom dictionary\n","thai_vocab.append('สามย่านมิตรทาวน์')\n","thai_vocab_trie = marisa_trie.Trie(thai_vocab)\n","txt_tokenized_tire = word_tokenize(text, engine='newmm', keep_whitespace=True, custom_dict=thai_vocab_trie)\n","print(\"custom dictionary with marisa_trie:\",txt_tokenized_tire)\n","\n","# Tokenizer object approach\n","from pythainlp.corpus.common import thai_words\n","from pythainlp import Tokenizer\n","\n","words = set(thai_words())\n","words.add(\"สามย่านมิตรทาวน์\")\n","custom_tokenizer = Tokenizer(words, engine='newmm')\n","print(\"custom dictionary with Tokenizer:\", custom_tokenizer.word_tokenize(text))\n","\n","######################"]}],"metadata":{"colab":{"collapsed_sections":["DF5Pme7CK3YF","xzs0R06q87Et","ZornooGF87Ew","w7vBXfjM87Ew","q0MJkKsh87Ex","57rP9cTU87Ez","Kpjwzw1w87E0","BSqLuK7G87E0","VLGgO8PrLSz6","LrZrzQoXLeUX","2SlX5cEBMHPd"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"40660bc5df986f346734aab38015efef98fb1def369e7c9aee8d6f0e0b3a1f56"}}},"nbformat":4,"nbformat_minor":0}
